{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import deque\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom tqdm.notebook import trange","metadata":{"_uuid":"6e945892-a7a1-4888-815b-17deeddb9dd4","_cell_guid":"42616efa-0613-43d8-af5f-ed81d949ccb9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-29T20:18:57.382204Z","iopub.execute_input":"2025-11-29T20:18:57.382484Z","iopub.status.idle":"2025-11-29T20:19:00.957433Z","shell.execute_reply.started":"2025-11-29T20:18:57.382462Z","shell.execute_reply":"2025-11-29T20:19:00.956824Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Resnet(nn.Module):\n\n    def __init__(self, in_channels, out_channels, resnet_blocks = 5):\n        super(Resnet, self).__init__()\n        self.start = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n        self.blocks = nn.ModuleList(\n            [ResidualConnection(out_channels) for _ in range(resnet_blocks)]\n        )\n\n        self.policy_head = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels = 32, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 6 * 7, 7)\n        )\n\n        self.value_head = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels = 3, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(3),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3 * 6 * 7, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.start(x)\n        for block in self.blocks:\n            x = block(x)\n        policy = self.policy_head(x)\n        value = self.value_head(x)\n        return policy, value\n        \n\nclass ResidualConnection(nn.Module):\n\n    def __init__(self, out_channels):\n        super(ResidualConnection, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels)     \n        )\n\n    def forward(self, x):\n        features = self.block(x)\n        return torch.relu(features + x)","metadata":{"_uuid":"44e627b8-d22a-4235-919f-a7ce4bb8de54","_cell_guid":"e4a0e432-46ac-47f3-af11-d00ecdc9c1c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-29T20:19:50.395093Z","iopub.execute_input":"2025-11-29T20:19:50.395808Z","iopub.status.idle":"2025-11-29T20:19:50.404084Z","shell.execute_reply.started":"2025-11-29T20:19:50.395783Z","shell.execute_reply":"2025-11-29T20:19:50.403225Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Connect4:\n\n    def __init__(self):\n        self.W = 7\n        self.H = 6\n        self.board = np.zeros((self.H, self.W))\n        self.all_moves = np.arange(self.W)\n\n\n    def reset(self):\n        self.board = np.zeros((self.H, self.W))\n        return self.board\n        \n    def get_valid_moves(self, state):\n        return (state[0] == 0).astype(int)\n\n    def step(self, move, player):\n        row, col = move//self.W, move%self.W\n\n        brow, bcol = self.H-1, col\n        while True:\n            if self.board[brow, bcol] == 0:\n                self.board[brow, bcol] = player\n                break\n            brow -= 1\n        result1 = self.check_winner(1)\n        result2 = self.check_winner(-1)\n\n        result = result1 or result2\n        \n        if result is not None:\n            done = True\n        elif not(self.board == 0).any():\n            done = True\n        else:\n            done = False\n\n        return self.board, result, done\n\n    def check_winner(self, player):\n        for r in range(self.H):\n            for c in range(self.W -3):\n                if self.board[r,c] == self.board[r, c + 1] == self.board[r, c+2] == self.board[r, c+3] == player:\n                    return player\n                    \n        for r in range(self.H - 3):\n            for c in range(self.W):\n                if self.board[r,c] == self.board[r + 1, c] == self.board[r + 2, c] == self.board[r+3, c] == player:\n                    return player\n\n        for r in range(self.H - 3):\n            for c in range(self.W -3):\n                if self.board[r,c] == self.board[r + 1, c + 1] == self.board[r + 2, c+2] == self.board[r + 3, c+3] == player:\n                    return player\n                    \n        for r in range(3, self.H):\n            for c in range(self.W -3):\n                if self.board[r,c] == self.board[r - 1, c + 1] == self.board[r - 2, c+2] == self.board[r - 3, c+3] == player:\n                    return player\n\n        return None\n        \n    def stackedStates(self, state, current_player):\n        \n        return np.stack((\n            state == current_player, \n            state == -current_player,\n            state == 0\n        )).astype(np.float32)\n\n    \n    def show(self, state):\n        print(state)","metadata":{"_uuid":"77093699-140b-48dc-b1c3-01b1f30369a4","_cell_guid":"efb70e81-7ac1-46e3-9a99-1ce1a9fe3f44","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-29T20:19:50.866175Z","iopub.execute_input":"2025-11-29T20:19:50.866697Z","iopub.status.idle":"2025-11-29T20:19:50.876902Z","shell.execute_reply.started":"2025-11-29T20:19:50.866671Z","shell.execute_reply":"2025-11-29T20:19:50.876063Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\n\nclass Node:\n\n    def __init__(self, state, parent, move, player, prob = 0):\n        self.H = 6\n        self.W = 7\n        self.state = state\n        self.parent = parent\n        self.move = move\n\n        self.player = player\n        self.children = {}\n        \n        # self.unexpanded_children = np.argwhere((self.state.flatten() == 0)).flatten()\n        self.prob = prob\n        \n        self.N = 0\n        self.W = 0\n\n# can optimize it\n    def is_terminal(self, H = 6, W = 7):\n\n        for r in range(H):\n            for c in range(W - 3):\n                val = self.state[r, c]\n                if val in [1, -1] and val == self.state[r, c+1] == self.state[r, c+2] == self.state[r, c+3]:\n                    return True, val\n        \n        # p\n        for r in range(H - 3):\n            for c in range(W):\n                val = self.state[r, c]\n                if val in [1, -1] and val == self.state[r+1, c] == self.state[r+2, c] == self.state[r+3, c]:\n                    return True, val\n        \n        # Diagonal \\ (down-right)\n        for r in range(H - 3):\n            for c in range(W - 3):\n                val = self.state[r, c]\n                if val in [1, -1] and val == self.state[r+1, c+1] == self.state[r+2, c+2] == self.state[r+3, c+3]:\n                    return True, val\n        \n        # Diagonal / (up-right)\n        for r in range(3, H):\n            for c in range(W - 3):\n                val = self.state[r, c]\n                if val in [1, -1] and val == self.state[r-1, c+1] == self.state[r-2, c+2] == self.state[r-3, c+3]:\n                    return True, val\n        \n        # return False\n        if not (self.state == 0).any():\n            return True, 0\n            \n        return False, 0\n\n    def is_fully_expanded(self):\n        # print(\"hereeeeee\")\n        # print(self.unexpanded_children)\n        # print(len(self.unexpanded_children) == 0)\n        # print((np.argwhere(self.state[0] == 0)))\n        return len(np.argwhere(self.state[0] == 0)) == 0\n\n    \n    def UCB1(self, c = 2):\n        if self.N == 0:\n            return float('inf')\n        return self.W/self.N + c * self.prob * np.sqrt(self.parent.N)/(self.N + 1)","metadata":{"_uuid":"6c08f6ea-4440-44a5-bfb0-a564b9a32718","_cell_guid":"b804da67-d946-4baa-b510-0e446ed55d0d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-29T20:19:51.146532Z","iopub.execute_input":"2025-11-29T20:19:51.146905Z","iopub.status.idle":"2025-11-29T20:19:51.156392Z","shell.execute_reply.started":"2025-11-29T20:19:51.146883Z","shell.execute_reply":"2025-11-29T20:19:51.155776Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MCTS:\n\n    def __init__(self, env, root_state, root_player, model, device):\n        self.env = env\n        self.H = 6\n        self.W = 7\n        self.policy = 0\n        self.model = model\n        self.device = device\n        self.root = Node(\n            state = root_state,\n            parent = None,\n            move = None,\n            player = root_player)\n        \n    def selection(self):\n\n        current = self.root\n\n        while True:\n            is_terminal, _ = current.is_terminal()\n            if is_terminal:\n                return current\n\n            if current.is_fully_expanded():\n                return current\n            if len(current.children) == 0:\n                return current\n            best_child = max(current.children.values(), key = lambda c: c.UCB1())\n            # print(best_child)\n            # max_UCB = 0\n            # for move, child in current.children.items():\n            #     if child.UCB1() > max_UCB:\n            #         max_UCB = child.UCB1()\n            #         best_child = child\n                    \n            current = best_child\n        # print(current)\n\n        return current\n\n    def expansion(self, node, policy):\n        # print(policy)\n        valid_states = self.env.get_valid_moves(node.state)\n        # valid_moves = np.where(valid_states)\n        for action, prob in enumerate(policy):\n            if valid_states[action] == 1 and action not in node.children:\n            # print(len(node.unexpanded_children))\n                # if len(node.unexpanded_children)>0 and not node.is_terminal():\n                    \n                    # print(action, prob)\n\n                # print(action, prob)\n                new_state = self.step(node.state, action, node.player)[0]\n                \n                child = Node(state = new_state,\n                             parent = node,\n                             move = action,\n                             player = -node.player,\n                             prob = prob\n                            )\n    \n                \n                node.children[action] = child\n                    # node.unexpanded_children = node.unexpanded_children[node.unexpanded_children != action]\n            # np.delete(node.unexpanded_children, np.where(node.unexpanded_children == move))\n\n        # return child\n        # return None\n\n\n    # def simulation(self, node):\n    #     policy, value = self.model(data)\n    #     return \n        # state = node.state.copy()\n        # current_player = node.player\n        \n        # while True:\n            \n        #     winner1 = self.check_winner(state, 1)\n        #     winner2 = self.check_winner(state, -1)\n\n        #     winner = winner1 or winner2\n            \n            \n        #     if winner is not None:\n        #         return winner, state\n        #     legal_moves = np.argwhere((state.flatten() == 0)).flatten()\n        #     if len(legal_moves) == 0:\n        #         return 0, state\n        #     move = np.random.choice(legal_moves)\n        #     state, winner = self.step(state, move, current_player)\n\n        #     if winner is not None:\n        #         return winner, state\n            \n        #     current_player = -current_player\n\n\n    def backpropagation(self, child, value):\n        current = child\n        while current is not None:\n            current.N += 1\n            current.W += value\n            value = -value\n            current = current.parent\n\n        # print(f\"Child : {child}\\nMove : {child.move}\\n Number : {child.N}\\nWin : {child.W}\")\n        \n        # current = child\n        # how_deep = 0\n        # while current is not None:\n\n        #     current.N += 1\n        #     # print(f\"Current State : \\n{current.state}\")\n        #     if winner == -current.player:\n        #         # print(f\"Winner : {winner}, Current Player : {current.player}\\nMove : {current.move} --> Plus One Win\\n\\n\")\n        #         current.W += 1\n        #     elif winner == 0:\n        #         # print(f\"Winner : {winner}, Current Player : {current.player}\\nMove : {current.move} --> Draw (Plus 0.5)\\n\\n\")\n        #         current.W += 0.5\n        #     # else:\n        #         # print(f\"Winner : {winner}, Current Player : {current.player}\\nMove : {current.move} --> No Win (Plus 0)\\n\\n\")\n                \n\n        #     current = current.parent\n\n    # @torch.no_grad()\n    # def search(self, iterations=10):\n\n    #     for _ in range(iterations):\n    #         # Select child based on UCB\n    #         child = self.selection()\n\n    #         # get stacked states\n    #         is_terminal, value = child.is_terminal()\n    #         print(is_terminal, value)\n\n    #         # GET THE VVALUES IMPROVE LATER\n    #         if not is_terminal:\n    #             stackedStates = self.env.stackedStates(child.state, child.player)\n    \n    #             # get policy and value from model\n    #             policy, value = self.model(torch.tensor(stackedStates, dtype = torch.float).unsqueeze(0).to(self.device))\n    \n    #             # turn them into probs\n    #             policy = torch.softmax(policy, dim = 1).squeeze(0).cpu().detach().numpy()\n    #             value = value.squeeze(0).cpu().detach().numpy()\n    \n    #             # Mask all the invalid moves\n    #             policy = policy * self.env.get_valid_moves(child.state)\n    \n    #             # Normalize after masking\n    #             policy = policy/(np.sum(policy))\n    \n                \n    #             self.expansion(child, policy)\n    #             # if len(child.children) >= 0:\n                    \n    #         # if current is None:\n\n    #                     #     current = child\n\n    #         # winner, state = self.simulation(current)\n    #         self.backpropagation(child, value)\n    #         # self.policy = policy\n\n    #     actions = np.zeros(7)\n\n\n    #     if len(self.root.children) == 0:\n    #         print(\"here\")\n    #         valid_moves = self.env.get_valid_moves(self.root.state)\n    #         action_probs = valid_moves / np.sum(valid_moves)\n    #         print(action_probs)\n    #         return action_probs\n    #     print(\"heehehe\")\n        \n    #     for action, child in self.root.children.items():\n    #         # print(action, child, child.N)\n    #         actions[action] = child.N\n    #         # print(actions)\n    #     actions /= np.sum(actions)\n    #     print(actions)\n    #     return actions\n    @torch.no_grad()\n    def search(self, iterations=10):\n    \n        for _ in range(iterations):\n            # Select child based on UCB\n            node = self.selection()\n    \n            # get stacked states\n            is_terminal, value = node.is_terminal()\n            if is_terminal:\n                value = value * node.player\n                \n            # print(is_terminal, value)\n    \n            # GET THE VALUES IMPROVE LATER\n            else:\n                stackedStates = self.env.stackedStates(node.state, node.player)\n    \n                # get policy and value from model\n                policy, value = self.model(torch.tensor(stackedStates, dtype = torch.float).unsqueeze(0).to(self.device))\n                policy = torch.softmax(policy, dim = 1).squeeze(0).cpu().detach().numpy()\n\n                if node is self.root:\n                    alpha = 0.3\n                    eps = 0.25\n                    noise = np.random.dirichlet([alpha] * len(self.env.all_moves))\n                    policy = (1 - eps) * policy + eps * noise\n                # turn them into probs\n                value = value.squeeze(0).cpu().item()\n    \n                # Mask all the invalid moves\n                valid_moves = self.env.get_valid_moves(node.state)\n                policy = policy * valid_moves\n    \n                # Normalize after masking\n                if np.sum(policy)>0:\n                    \n                    policy = policy/(np.sum(policy))\n                else:\n                    policy = valid_moves/np.sum(valid_moves)\n    \n                # Expand the node (creates children)\n                self.expansion(node, policy)\n                \n                # *** KEY FIX: Pick one of the newly created children to backprop from ***\n                if len(node.children) > 0:\n                    best_action = max(node.children.keys(), key=lambda a: node.children[a].prob)\n                    child = node.children[best_action]\n                else:\n                    child = node\n            # else:\n            #     child = node\n\n            target_node = node if is_terminal else child\n    \n            # Backpropagate from the CHILD, not the parent\n            self.backpropagation(target_node, value)\n        actions = np.zeros(7)\n\n        if len(self.root.children) == 0:\n            # print(\"here\")\n            valid_moves = self.env.get_valid_moves(self.root.state)\n            action_probs = valid_moves / np.sum(valid_moves)\n            # print(action_probs)\n            return action_probs\n        # print(\"heehehe\")\n        total_visits = sum(child.N for child in self.root.children.values())\n        if total_visits > 0:\n            \n            for action, child in self.root.children.items():\n            # print(action, child, child.N)\n                actions[action] = child.N\n            # print(actions)\n        # if total_visits > 0:\n            actions /= total_visits\n            \n        \n        \n        else:\n            valid_moves = self.env.get_valid_moves(self.root.state)\n            actions = valid_moves / np.sum(valid_moves)\n        \n        \n        # print(actions)\n        return actions\n\n\n\n    \n    def __repr__(self):\n        return f\"{self.root.state}\\n{self.root.parent}\\n{self.root.children}\\n{self.root.action}\\n{self.root.children.n}\"\n\n    def train(self):\n        P, value = self.network(self.board)\n        loss = self.loss(qValue, target_qValues)\n        # self.loss_value += loss.item()\n        self.optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n    \n    @staticmethod\n    def check_winner(state, player, H = 6, W = 7):\n        if not (state == 0).any():\n            return 0\n\n        for r in range(H):\n            for c in range(W -3):\n                if state[r,c] == state[r, c + 1] == state[r, c+2] == state[r, c+3] == player:\n                    return player\n                    \n        for r in range(H - 3):\n            for c in range(W):\n                if state[r,c] == state[r + 1, c] == state[r + 2, c] == state[r+3, c] == player:\n                    return player\n\n        for r in range(H - 3):\n            for c in range(W -3):\n                if state[r,c] == state[r + 1, c + 1] == state[r + 2, c+2] == state[r + 3, c+3] == player:\n                    return player\n                    \n        for r in range(3, H):\n            for c in range(W -3):\n                if state[r,c] == state[r - 1, c + 1] == state[r - 2, c+2] == state[r - 3, c+3] == player:\n                    return player\n\n        return None\n        \n    @staticmethod\n    def step(state, action, player, H = 6, W = 7):\n        state = state.copy()\n\n\n        col = action%W\n        # self.board[row, col] = 1\n        # print(move, row, col)\n        brow, bcol = H-1, col\n        \n        while True:\n            # print(brow, bcol)\n            # print(state)\n            if state[brow, bcol] == 0:\n                state[brow, bcol] = player\n                break\n            brow -= 1\n        result1 = MCTS.check_winner(state, 1)\n        result2 = MCTS.check_winner(state, -1)\n\n        result = result1 or result2\n        \n        if result is not None:\n            done = True\n        elif not(state == 0).any():\n            done = True\n        else:\n            done = False\n\n        return state, result, done\n\n        # self.player = -self.player\n        # print(\"Ended From Here\")\n        # return state, result","metadata":{"_uuid":"3c8f83ee-791d-4b82-bc3c-eb5768fc8853","_cell_guid":"936ec8e4-32b2-4f12-86ed-02526e46eba1","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:19:51.535158Z","iopub.execute_input":"2025-11-29T20:19:51.535799Z","iopub.status.idle":"2025-11-29T20:19:51.557132Z","shell.execute_reply.started":"2025-11-29T20:19:51.535772Z","shell.execute_reply":"2025-11-29T20:19:51.556213Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AlphaZero:\n\n    def __init__(self, env, model, policy_loss, value_loss, optimizer, device, current_player = 1):\n        self.env = env\n        self.model = model\n        self.model.to(device)\n        self.optimizer = optimizer\n        self.mainBuffer = deque(maxlen = 30000)\n        self.policy_loss = policy_loss\n        self.value_loss = value_loss\n        self.device = device\n        self.current_player = current_player\n        self.tau = 1.25\n\n    \n    def selfPlayData(self, ):\n        buffer = []\n        state = self.env.reset()\n        current_player = 1\n        \n        while True:\n            root = MCTS(self.env, state, current_player, self.model, self.device)\n\n            actions = root.search(iterations = 100)\n            temperature_probs = actions ** (1/self.tau)\n            temperature_probs /= np.sum(temperature_probs)\n            buffer.append((state, actions, current_player))\n\n            # if np.random.random() < 0.1:\n            #     # print(\"There\")\n            #     valid_moves = self.env.get_valid_moves(state)\n            #     # print(valid_moves)\n            #     valid_probs = valid_moves/np.sum(valid_moves)\n            #     action = np.random.choice(len(valid_moves), p = valid_probs)\n            # else:\n                # print(\"h1ere\")\n                # print(actions)\n            \n            action = np.random.choice(self.env.all_moves, p = temperature_probs)\n            # print(actions)\n            # print(action)\n            # print(state)\n            \n            state, result, done = root.step(state, action, current_player)\n            if done:\n                mainBuffer = []\n                for states, actions, player in buffer:\n                    if result == 0:\n                        value = 0\n                    elif result == player:\n                        value = 1\n                    else:\n                        value = -1\n                    mainBuffer.append((self.env.stackedStates(states, current_player), actions, value))\n                return mainBuffer\n\n            current_player = -current_player\n\n    def train(self, selfPlayIterations, epochs, batch_size, total_iterations = 50):\n        for i in trange(total_iterations):\n            self.model.eval()\n            self.mainBuffer = []\n            for _ in trange(selfPlayIterations):\n                self.mainBuffer.extend(self.selfPlayData())\n\n            \n            self.model.train()\n            losses = []\n            epoch_loss = 0\n            previous_model = self.model\n            for epoch in trange(epochs):\n\n                epoch_loss = 0\n                batch_count = 0\n                np.random.shuffle(self.mainBuffer)\n                for idx in range(0, len(self.mainBuffer), batch_size):\n                    data = self.mainBuffer[idx:idx+batch_size]\n                    states, actions, value = zip(*data)\n    \n                    actions = torch.tensor(actions, dtype = torch.float).to(self.device)\n                    values = torch.tensor(value, dtype = torch.float).unsqueeze(1).to(self.device)\n                    states = torch.tensor(states, dtype = torch.float).to(self.device)\n\n                    model_policy, model_value = self.model(states)\n\n                    policy_loss = self.policy_loss(model_policy, actions)\n                    value_loss = self.value_loss(model_value, values)\n                    total_loss = policy_loss + value_loss\n                    \n                    self.optimizer.zero_grad()\n                    total_loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    self.optimizer.step()            \n\n                    epoch_loss += total_loss.item()\n                    batch_count += 1\n\n                avg_loss = epoch_loss/batch_count\n                    \n                print(f\"Epoch {epoch + 1}\\tTotal Loss : {avg_loss}\")\n                losses.append(avg_loss)\n            \n            self.evaluation(previous_model)\n\n    def evaluation(self, pmodel,num_games = 20):\n        wins = 0\n        self.model.eval()\n\n        for game in range(num_games):\n                state = self.env.reset()\n                current_player = 1\n\n                while True:\n                    root = MCTS(self.env, state, current_player, self.model, self.device)\n                    actions = root.search(iterations = 100)\n                    action = np.argmax(actions)\n                    state, result, done = root.step(state, action, current_player)\n                    if done:\n                        if result == current_player:\n                            wins += 1\n                        break\n                    current_player = -current_player\n\n                    root = MCTS(self.env, state, current_player, pmodel, self.device)\n                    actions = root.search(iterations = 100)\n                    action = np.argmax(actions)\n                    state, result, done = root.step(state, action, current_player)\n                    if done:\n                        break\n                    current_player = -current_player\n\n        win_rate = wins/num_games\n        print(f\"Evaluation : {wins}/{num_games} Wins : {wins}\")\n                            ","metadata":{"_uuid":"aa2d5c34-18fe-424a-81a5-3c7b77c03752","_cell_guid":"bef1f947-1d4a-4c57-a67a-93cccfade916","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-29T20:19:51.645829Z","iopub.execute_input":"2025-11-29T20:19:51.646525Z","iopub.status.idle":"2025-11-29T20:19:51.659898Z","shell.execute_reply.started":"2025-11-29T20:19:51.646497Z","shell.execute_reply":"2025-11-29T20:19:51.659045Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"env = Connect4()\nmodel = Resnet(3, 64, 5)\noptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-4)\npolicy_loss = nn.CrossEntropyLoss()\nvalue_loss = nn.MSELoss()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nagent= AlphaZero(env, model, policy_loss, value_loss, optimizer, device)\nagent.train(500, 10, 128)","metadata":{"_uuid":"a56719df-2126-4eda-9fb4-917265e330c2","_cell_guid":"f9d2fe84-8173-42d5-b869-7756e0f863ff","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T20:19:52.275072Z","iopub.execute_input":"2025-11-29T20:19:52.275663Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b884b77b060c40fcabb4600a387ab109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c422cd049f014405899e760dfa3b1350"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"60ef7712-c26d-4db5-8ed0-b5065eb476ac","_cell_guid":"1e09eb26-d070-4896-9288-5e6656900bef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}