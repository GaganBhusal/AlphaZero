{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import deque\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom tqdm.notebook import trange","metadata":{"_uuid":"6e945892-a7a1-4888-815b-17deeddb9dd4","_cell_guid":"42616efa-0613-43d8-af5f-ed81d949ccb9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T21:03:27.387826Z","iopub.execute_input":"2025-11-30T21:03:27.388102Z","iopub.status.idle":"2025-11-30T21:03:27.392563Z","shell.execute_reply.started":"2025-11-30T21:03:27.388083Z","shell.execute_reply":"2025-11-30T21:03:27.391788Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class Resnet(nn.Module):\n\n    def __init__(self, in_channels, out_channels, resnet_blocks = 5):\n        super(Resnet, self).__init__()\n        self.start = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n        self.blocks = nn.ModuleList(\n            [ResidualConnection(out_channels) for _ in range(resnet_blocks)]\n        )\n\n        self.policy_head = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels = 32, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 6 * 7, 7)\n        )\n\n        self.value_head = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels = 3, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(3),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3 * 6 * 7, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.start(x)\n        for block in self.blocks:\n            x = block(x)\n        policy = self.policy_head(x)\n        value = self.value_head(x)\n        return policy, value\n        \n\nclass ResidualConnection(nn.Module):\n\n    def __init__(self, out_channels):\n        super(ResidualConnection, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm2d(out_channels)     \n        )\n\n    def forward(self, x):\n        features = self.block(x)\n        return torch.relu(features + x)","metadata":{"_uuid":"44e627b8-d22a-4235-919f-a7ce4bb8de54","_cell_guid":"e4a0e432-46ac-47f3-af11-d00ecdc9c1c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T21:03:27.399548Z","iopub.execute_input":"2025-11-30T21:03:27.399738Z","iopub.status.idle":"2025-11-30T21:03:27.411046Z","shell.execute_reply.started":"2025-11-30T21:03:27.399724Z","shell.execute_reply":"2025-11-30T21:03:27.410353Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class Connect4:\n\n    def __init__(self):\n        self.W = 7\n        self.H = 6\n        self.board = np.zeros((self.H, self.W))\n        self.all_moves = np.arange(self.W)\n\n\n    def reset(self):\n        self.board = np.zeros((self.H, self.W))\n        return self.board\n        \n    def get_valid_moves(self, state):\n        return (state[0] == 0).astype(int)\n\n    def step(self, board, move, player):\n        board = board.copy()\n        row, col = move//self.W, move%self.W\n        brow, bcol = self.H-1, col\n        while True:\n            if board[brow, bcol] == 0:\n                board[brow, bcol] = player\n                break\n            brow -= 1\n        done, result = self.is_terminal(board)\n        return board, result, done\n\n    def is_terminal(self, board,H = 6, W = 7):\n        for r in range(H):\n            for c in range(W - 3):\n                val = board[r, c]\n                if val in [1, -1] and val == board[r, c+1] == board[r, c+2] == board[r, c+3]:\n                    return True, val\n        for r in range(H - 3):\n            for c in range(W):\n                val = board[r, c]\n                if val in [1, -1] and val == board[r+1, c] == board[r+2, c] == board[r+3, c]:\n                    return True, val\n        for r in range(H - 3):\n            for c in range(W - 3):\n                val = board[r, c]\n                if val in [1, -1] and val == board[r+1, c+1] == board[r+2, c+2] == board[r+3, c+3]:\n                    return True, val\n        for r in range(3, H):\n            for c in range(W - 3):\n                val = board[r, c]\n                if val in [1, -1] and val == board[r-1, c+1] == board[r-2, c+2] == board[r-3, c+3]:\n                    return True, val\n        if not (board == 0).any():\n            return True, 0\n        return False, 0\n        \n    def stackedStates(self, state, current_player):\n        \n        return np.stack((\n            state == current_player, \n            state == -current_player,\n            state == 0\n        )).astype(np.float32)\n\n    \n    def show(self, state):\n        print(state)","metadata":{"_uuid":"77093699-140b-48dc-b1c3-01b1f30369a4","_cell_guid":"efb70e81-7ac1-46e3-9a99-1ce1a9fe3f44","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T21:03:27.411964Z","iopub.execute_input":"2025-11-30T21:03:27.412209Z","iopub.status.idle":"2025-11-30T21:03:27.428944Z","shell.execute_reply.started":"2025-11-30T21:03:27.412181Z","shell.execute_reply":"2025-11-30T21:03:27.428351Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import numpy as np\n\nclass Node:\n\n    def __init__(self, state, parent, move, player, prob = 0):\n        self.H = 6\n        self.W = 7\n        self.state = state\n        self.parent = parent\n        self.move = move\n\n        self.player = player\n        self.children = {}\n        \n        self.prob = prob\n        \n        self.N = 0\n        self.W = 0\n        \n    def is_fully_expanded(self):\n        return len(np.argwhere(self.state[0] == 0)) == 0\n\n    \n    def UCB1(self, c = 2):\n        if self.N == 0:\n            return float('inf')\n        return self.W/self.N + c * self.prob * np.sqrt(self.parent.N)/(self.N + 1)","metadata":{"_uuid":"6c08f6ea-4440-44a5-bfb0-a564b9a32718","_cell_guid":"b804da67-d946-4baa-b510-0e446ed55d0d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T21:03:27.506229Z","iopub.execute_input":"2025-11-30T21:03:27.506550Z","iopub.status.idle":"2025-11-30T21:03:27.514779Z","shell.execute_reply.started":"2025-11-30T21:03:27.506528Z","shell.execute_reply":"2025-11-30T21:03:27.513529Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class MCTS:\n\n    def __init__(self, env, root_state, root_player, model, device, PARAMS):\n        self.env = env\n        self.H = 6\n        self.W = 7\n        self.policy = 0\n        self.model = model\n        self.device = device\n        self.root = Node(state = root_state,\n                         parent = None,\n                         move = None,\n                         player = root_player\n                        )\n        self.PARAMS = PARAMS\n        \n    def selection(self):\n        current = self.root\n        \n        while True:\n            is_terminal, _ = self.env.is_terminal(current.state)\n            if is_terminal:\n                return current\n            if current.is_fully_expanded():\n                return current\n            if len(current.children) == 0:\n                return current\n            best_child = max(current.children.values(), key = lambda c: c.UCB1())\n            current = best_child\n            \n        return current\n\n    def expansion(self, node, policy):\n        valid_states = self.env.get_valid_moves(node.state)\n\n        for action, prob in enumerate(policy):\n            if valid_states[action] == 1 and action not in node.children:\n                new_state = self.env.step(node.state, action, node.player)[0]\n                child = Node(state = new_state,\n                             parent = node,\n                             move = action,\n                             player = -node.player,\n                             prob = prob\n                            )\n                node.children[action] = child\n                \n    def backpropagation(self, child, value):\n        current = child\n        while current is not None:\n            current.N += 1\n            current.W += value\n            value = -value\n            current = current.parent\n\n    @torch.no_grad()\n    def search(self):\n    \n        for _ in range(self.PARAMS['SEARCHES']):\n            node = self.selection()\n            is_terminal, value = self.env.is_terminal(node.state)\n            \n            if is_terminal:\n                value = value * node.player\n            else:\n                stackedStates = self.env.stackedStates(node.state, node.player)\n                policy, value = self.model(torch.tensor(stackedStates, dtype = torch.float).unsqueeze(0).to(self.device))\n                policy = torch.softmax(policy, dim = 1).squeeze(0).cpu().detach().numpy()\n                if node is self.root:\n                    alpha = self.PARAMS['ALPHA']\n                    eps = self.PARAMS['EPSILON']\n                    noise = np.random.dirichlet([alpha] * len(self.env.all_moves))\n                    policy = (1 - eps) * policy + eps * noise\n\n                value = value.squeeze(0).cpu().item()\n                valid_moves = self.env.get_valid_moves(node.state)\n                policy = policy * valid_moves\n                if np.sum(policy)>0:\n                    policy = policy/(np.sum(policy))\n                else:\n                    policy = valid_moves/np.sum(valid_moves)\n    \n                self.expansion(node, policy)\n                if len(node.children) > 0:\n                    best_action = max(node.children.keys(), key=lambda a: node.children[a].prob)\n                    child = node.children[best_action]\n                else:\n                    child = node\n            target_node = node if is_terminal else child\n    \n            self.backpropagation(target_node, value)\n        actions = np.zeros(7)\n\n        if len(self.root.children) == 0:\n            valid_moves = self.env.get_valid_moves(self.root.state)\n            action_probs = valid_moves / np.sum(valid_moves)\n            return action_probs\n\n        total_visits = sum(child.N for child in self.root.children.values())\n        if total_visits > 0:\n            for action, child in self.root.children.items():\n                actions[action] = child.N\n            actions /= total_visits\n        else:\n            valid_moves = self.env.get_valid_moves(self.root.state)\n            actions = valid_moves / np.sum(valid_moves)\n        return actions\n\n    def __repr__(self):\n        return f\"{self.root.state}\\n{self.root.parent}\\n{self.root.children}\\n{self.root.action}\\n{self.root.children.n}\"","metadata":{"_uuid":"3c8f83ee-791d-4b82-bc3c-eb5768fc8853","_cell_guid":"936ec8e4-32b2-4f12-86ed-02526e46eba1","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:03:27.518317Z","iopub.execute_input":"2025-11-30T21:03:27.519051Z","iopub.status.idle":"2025-11-30T21:03:27.537862Z","shell.execute_reply.started":"2025-11-30T21:03:27.519023Z","shell.execute_reply":"2025-11-30T21:03:27.537054Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class AlphaZero:\n\n    def __init__(self, env, model, policy_loss, value_loss, optimizer, device, PARAMS, current_player = 1):\n        self.env = env\n        self.model = model\n        self.model.to(device)\n        self.optimizer = optimizer\n        self.mainBuffer = deque(maxlen = 30000)\n        self.policy_loss = policy_loss\n        self.value_loss = value_loss\n        self.device = device\n        self.current_player = current_player\n        self.tau = 1.25\n        self.PARAMS = PARAMS\n\n    \n    def selfPlayData(self, ):\n        buffer = []\n        state = self.env.reset()\n        current_player = 1\n        \n        while True:\n            root = MCTS(self.env, state, current_player, self.model, self.device, self.PARAMS)\n\n            actions = root.search()\n            temperature_probs = actions ** (1/self.PARAMS['TAU'])\n            temperature_probs /= np.sum(temperature_probs)\n            buffer.append((state, actions, current_player))\n            action = np.random.choice(self.env.all_moves, p = temperature_probs)\n            state, result, done = self.env.step(state, action, current_player)\n            if done:\n                mainBuffer = []\n                for states, actions, player in buffer:\n                    if result == 0:\n                        value = 0\n                    elif result == player:\n                        value = 1\n                    else:\n                        value = -1\n                    mainBuffer.append((self.env.stackedStates(states, current_player), actions, value))\n                return mainBuffer\n\n            current_player = -current_player\n\n    def train(self):\n        for i in trange(self.PARAMS['TOTAL_ITERATIONS']):\n            self.model.eval()\n            self.mainBuffer = []\n            for _ in trange(self.PARAMS['SELF_PLAY_ITERATIONS']):\n                self.mainBuffer.extend(self.selfPlayData())\n            \n            self.model.train()\n            losses = []\n            epoch_loss = 0\n            previous_model = self.model\n            for epoch in trange(self.PARAMS['EPOCHS']):\n\n                epoch_loss = 0\n                batch_count = 0\n                np.random.shuffle(self.mainBuffer)\n                for idx in range(0, len(self.mainBuffer), self.PARAMS[\"BATCH_SIZE\"]):\n                    data = self.mainBuffer[idx:idx+self.PARAMS[\"BATCH_SIZE\"]]\n                    states, actions, value = zip(*data)\n    \n                    actions = torch.tensor(actions, dtype = torch.float).to(self.device)\n                    values = torch.tensor(value, dtype = torch.float).unsqueeze(1).to(self.device)\n                    states = torch.tensor(states, dtype = torch.float).to(self.device)\n\n                    model_policy, model_value = self.model(states)\n\n                    policy_loss = self.policy_loss(model_policy, actions)\n                    value_loss = self.value_loss(model_value, values)\n                    total_loss = policy_loss + value_loss\n                    \n                    self.optimizer.zero_grad()\n                    total_loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    self.optimizer.step()            \n\n                    epoch_loss += total_loss.item()\n                    batch_count += 1\n\n                avg_loss = epoch_loss/batch_count\n                    \n                print(f\"Epoch {epoch + 1}\\tTotal Loss : {avg_loss}\")\n                losses.append(avg_loss)\n            \n            self.evaluation(previous_model)\n            self.save_model()\n\n    def save_model(self):\n\n        checkpoint = {\n            \"model\" : self.model.state_dict(),\n            \"optimizer_state\" : self.optimizer.state_dict()\n        }\n        torch.save(checkpoint, 'checkpoint.pth')\n        print(\"Hit Checkpoint\")\n        \n        \n    \n\n    def evaluation(self, pmodel):\n        wins = 0\n        self.model.eval()\n\n        for game in trange(self.PARAMS['EVALUATION_GAMES']):\n                state = self.env.reset()\n                current_player = 1\n\n                while True:\n                    root = MCTS(self.env, state, current_player, self.model, self.device, self.PARAMS)\n                    actions = root.search()\n                    action = np.argmax(actions)\n                    state, result, done = self.env.step(state, action, current_player)\n                    if done:\n                        if result == current_player:\n                            wins += 1\n                        break\n                    current_player = -current_player\n\n                    root = MCTS(self.env, state, current_player, pmodel, self.device, self.PARAMS)\n                    actions = root.search()\n                    action = np.argmax(actions)\n                    state, result, done = self.env.step(state, action, current_player)\n                    if done:\n                        break\n                    current_player = -current_player\n\n        win_rate = wins/self.PARAMS[\"EVALUATION_GAMES\"]\n        print(f\"Evaluation : {wins}/{self.PARAMS['EVALUATION_GAMES']} Wins : {wins}\")\n                            ","metadata":{"_uuid":"aa2d5c34-18fe-424a-81a5-3c7b77c03752","_cell_guid":"bef1f947-1d4a-4c57-a67a-93cccfade916","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T21:20:43.733283Z","iopub.execute_input":"2025-11-30T21:20:43.733810Z","iopub.status.idle":"2025-11-30T21:20:43.749341Z","shell.execute_reply.started":"2025-11-30T21:20:43.733783Z","shell.execute_reply":"2025-11-30T21:20:43.748484Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"PARAMETERS = {\n    \n    \"IN_CHANNELS\" : 3,\n    \"OUT_CHANNELS\" : 64,\n    \"RESNET_BLOCKS\" : 5,\n    \n    \"SELF_PLAY_ITERATIONS\" : 500,\n    \"EPOCHS\" : 20,\n    \"BATCH_SIZE\" : 128,\n    \"TOTAL_ITERATIONS\" : 20,\n    \n    \"SEARCHES\" : 100,\n    \"EVALUATION_GAMES\" : 20,\n    \n    \"ALPHA\" : 0.3,\n    \"EPSILON\" : 0.25,\n    \"TAU\" : 1.25\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:20:44.886143Z","iopub.execute_input":"2025-11-30T21:20:44.886823Z","iopub.status.idle":"2025-11-30T21:20:44.890516Z","shell.execute_reply.started":"2025-11-30T21:20:44.886801Z","shell.execute_reply":"2025-11-30T21:20:44.889832Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"env = Connect4()\n\nmodel = Resnet(PARAMETERS['IN_CHANNELS'], \n               PARAMETERS['OUT_CHANNELS'], \n               PARAMETERS['RESNET_BLOCKS']\n              )\n\noptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-4)\npolicy_loss = nn.CrossEntropyLoss()\nvalue_loss = nn.MSELoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nagent= AlphaZero(env, \n                 model, \n                 policy_loss, \n                 value_loss, \n                 optimizer, \n                 device, \n                 PARAMETERS\n                )\n\nagent.train()","metadata":{"_uuid":"a56719df-2126-4eda-9fb4-917265e330c2","_cell_guid":"f9d2fe84-8173-42d5-b869-7756e0f863ff","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:20:45.411555Z","iopub.execute_input":"2025-11-30T21:20:45.412027Z","iopub.status.idle":"2025-11-30T21:22:09.452929Z","shell.execute_reply.started":"2025-11-30T21:20:45.412005Z","shell.execute_reply":"2025-11-30T21:22:09.452125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86898bbbdcff45be9939e5b38e89d504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f210a2ee4511401abc92631b21427e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9319c967404663a2863c3d2a3b119b"}},"metadata":{}},{"name":"stdout","text":"Epoch 1\tTotal Loss : 3.398463726043701\nEpoch 2\tTotal Loss : 2.948635458946228\nEpoch 3\tTotal Loss : 2.8905352354049683\nEpoch 4\tTotal Loss : 2.8924070596694946\nEpoch 5\tTotal Loss : 2.837430953979492\nEpoch 6\tTotal Loss : 2.8480865955352783\nEpoch 7\tTotal Loss : 2.816851854324341\nEpoch 8\tTotal Loss : 2.773837447166443\nEpoch 9\tTotal Loss : 2.7471930980682373\nEpoch 10\tTotal Loss : 2.686057925224304\nEpoch 11\tTotal Loss : 2.7202560901641846\nEpoch 12\tTotal Loss : 2.636173129081726\nEpoch 13\tTotal Loss : 2.5652647018432617\nEpoch 14\tTotal Loss : 2.509036064147949\nEpoch 15\tTotal Loss : 2.407310366630554\nEpoch 16\tTotal Loss : 2.412977695465088\nEpoch 17\tTotal Loss : 2.284764528274536\nEpoch 18\tTotal Loss : 2.2325801849365234\nEpoch 19\tTotal Loss : 2.1369320154190063\nEpoch 20\tTotal Loss : 2.116098165512085\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cb6e4d83064c3e9b641a04a292cfaf"}},"metadata":{}},{"name":"stdout","text":"Evaluation : 5/5 Wins : 5\nHit Checkpoint\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e675798f054bce92b88fd8518e201c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a145094917db4a409e0f7447f6014bff"}},"metadata":{}},{"name":"stdout","text":"Epoch 1\tTotal Loss : 3.3337481021881104\nEpoch 2\tTotal Loss : 2.828295946121216\nEpoch 3\tTotal Loss : 2.6811617612838745\nEpoch 4\tTotal Loss : 2.4485028982162476\nEpoch 5\tTotal Loss : 2.4347630739212036\nEpoch 6\tTotal Loss : 2.2462315559387207\nEpoch 7\tTotal Loss : 2.1551326513290405\nEpoch 8\tTotal Loss : 2.1901028156280518\nEpoch 9\tTotal Loss : 1.9728506803512573\nEpoch 10\tTotal Loss : 2.0071810483932495\nEpoch 11\tTotal Loss : 1.9085388779640198\nEpoch 12\tTotal Loss : 1.8616562485694885\nEpoch 13\tTotal Loss : 2.029250681400299\nEpoch 14\tTotal Loss : 1.983208954334259\nEpoch 15\tTotal Loss : 2.1405811309814453\nEpoch 16\tTotal Loss : 1.882051706314087\nEpoch 17\tTotal Loss : 1.9679740071296692\nEpoch 18\tTotal Loss : 1.8436037302017212\nEpoch 19\tTotal Loss : 1.7245543599128723\nEpoch 20\tTotal Loss : 1.742594838142395\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac33020afea541f2bd500487d5589289"}},"metadata":{}},{"name":"stdout","text":"Evaluation : 3/5 Wins : 3\nHit Checkpoint\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"_uuid":"60ef7712-c26d-4db5-8ed0-b5065eb476ac","_cell_guid":"1e09eb26-d070-4896-9288-5e6656900bef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}